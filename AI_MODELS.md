# AI Models sá»­ dá»¥ng trong HÃ nh TrÃ¬nh Chá»¯ S

## Tá»•ng quan

Project sá»­ dá»¥ng **GPT-4o-mini** qua nhÃ  cung cáº¥p **MegaLLM** Ä‘á»ƒ sinh cÃ¢u há»i tráº¯c nghiá»‡m tá»± Ä‘á»™ng cho 63 tá»‰nh thÃ nh Viá»‡t Nam.

## Chi tiáº¿t Models

### 1. GPT-4o-mini via MegaLLM (Text Generation)

**Má»¥c Ä‘Ã­ch**: Sinh cÃ¢u há»i tráº¯c nghiá»‡m vá» Ä‘á»‹a lÃ½, lá»‹ch sá»­, vÄƒn hÃ³a cá»§a tá»«ng tá»‰nh

**Model ID**: `gpt-4o-mini`

**NhÃ  cung cáº¥p**: MegaLLM (https://ai.megallm.io)

**Táº¡i sao chá»n GPT-4o-mini?**
- âš¡ **Nhanh**: Pháº£n há»“i nhanh vÃ  hiá»‡u quáº£
- ðŸ’° **Tiáº¿t kiá»‡m**: Chi phÃ­ tháº¥p cho structured output
- ðŸŽ¯ **ChÃ­nh xÃ¡c**: Cháº¥t lÆ°á»£ng cao cho task sinh cÃ¢u há»i
- ðŸ”¥ **OpenAI Compatible**: Sá»­ dá»¥ng OpenAI SDK chuáº©n

**SDK**: `openai` (OpenAI official SDK)

**Cáº¥u hÃ¬nh**:
```javascript
{
  model: 'gpt-4o-mini',
  temperature: 1.0,      // Creativity cao Ä‘á»ƒ cÃ¢u há»i Ä‘a dáº¡ng, ngáº«u nhiÃªn
  max_tokens: 2048
}
```

**Input**: TÃªn tá»‰nh (VD: "HÃ  Ná»™i")

**Output**: JSON chá»©a 5 cÃ¢u há»i, má»—i cÃ¢u cÃ³:
- `question`: CÃ¢u há»i báº±ng tiáº¿ng Viá»‡t
- `options`: Máº£ng 4 Ä‘Ã¡p Ã¡n
- `correctAnswer`: ÄÃ¡p Ã¡n Ä‘Ãºng (pháº£i khá»›p vá»›i má»™t trong 4 options)

**VÃ­ dá»¥ Output**:
```json
{
  "questions": [
    {
      "question": "Äáº·c sáº£n ná»•i tiáº¿ng cá»§a HÃ  Ná»™i lÃ  gÃ¬?",
      "options": ["Phá»Ÿ", "BÃºn cháº£", "BÃ¡nh cuá»‘n", "Cháº£ cÃ¡"],
      "correctAnswer": "Phá»Ÿ"
    }
  ]
}
```

**Prompt Engineering**:
- YÃªu cáº§u sinh cÃ¢u há»i theo chá»§ Ä‘á»: Ä‘á»‹a lÃ½, lá»‹ch sá»­, vÄƒn hÃ³a, Ä‘áº·c sáº£n, Ä‘á»‹a danh
- Äáº£m báº£o táº¥t cáº£ text báº±ng tiáº¿ng Viá»‡t cÃ³ dáº¥u
- Validation: correctAnswer pháº£i khá»›p chÃ­nh xÃ¡c vá»›i options
- Format: JSON thuáº§n (khÃ´ng markdown)

**Error Handling**:
- Náº¿u API fails â†’ Fallback vá» mock data
- Náº¿u JSON parse lá»—i â†’ Fallback vá» mock data
- Náº¿u validation fails â†’ Throw error vÃ  fallback

**Chi phÃ­ Æ°á»›c tÃ­nh**:
- ~$0.0001-0.0005 per request (5 questions) - Ráº» hÆ¡n nhiá»u!
- Vá»›i caching 24h: ~$2-5/thÃ¡ng cho 1000+ users
- Flash ráº» hÆ¡n Pro khoáº£ng 10x

---

### 2. Image Generation (Skipped cho MVP)

**Tráº¡ng thÃ¡i**: ChÆ°a triá»ƒn khai

**LÃ½ do**:
- Google Gemini khÃ´ng cÃ³ public API cho image generation
- Imagen API chÆ°a widely available
- MVP táº­p trung vÃ o gameplay, khÃ´ng cáº§n áº£nh báº¯t buá»™c

**TÆ°Æ¡ng lai**:
CÃ³ thá»ƒ tÃ­ch há»£p:
- **Google Imagen API** (khi available)
- **Stable Diffusion API** (via Replicate, Hugging Face)
- **DALL-E API** (OpenAI)
- **Midjourney API** (khi available)

Æ¯á»›c tÃ­nh chi phÃ­: +$10-20/thÃ¡ng

---

## Caching & Optimization

### In-Memory Cache
- **TTL**: 24 giá»
- **Key**: TÃªn tá»‰nh
- **Purpose**: Giáº£m 80-90% API calls sau warm-up
- **Storage**: Map trong Node.js process

### Rate Limiting
- **Limit**: 10 requests/60 giÃ¢y per province
- **Purpose**: TrÃ¡nh abuse, kiá»ƒm soÃ¡t chi phÃ­
- **Implementation**: Simple in-memory counter

---

## API Key Setup

### Láº¥y API Key:
1. Truy cáº­p: https://ai.megallm.io
2. ÄÄƒng kÃ½/ÄÄƒng nháº­p tÃ i khoáº£n
3. Táº¡o API key má»›i
4. Copy vÃ  paste vÃ o `.env.local`

### Environment Variable:
```bash
MEGALLM_API_KEY=your-api-key-here
```

### Security:
- âš ï¸ **KHÃ”NG** commit API key lÃªn Git
- âš ï¸ **KHÃ”NG** expose API key á»Ÿ client-side
- âœ… Chá»‰ sá»­ dá»¥ng trong Server Actions (`'use server'`)
- âœ… Add `.env.local` vÃ o `.gitignore`

---

## Performance & Monitoring

### Metrics cáº§n theo dÃµi:
- **API Response Time**: ~2-5 giÃ¢y (Gemini 1.5 Pro)
- **Cache Hit Rate**: Target >80%
- **Error Rate**: Target <5%
- **Cost per Day**: Target <$0.50

### Logging:
```
[CACHE HIT] Quiz for HÃ  Ná»™i     // Cache hit
[CACHE MISS] Generating quiz    // API call
[ERROR] Failed to generate      // Fallback to mock
```

---

## Roadmap

### Phase 1 (MVP - Current):
- âœ… Gemini 2.0 Flash cho text generation
- âœ… Mock data fallback
- âœ… Basic caching (24h TTL)
- âœ… Rate limiting (10 req/min)

### Phase 2 (Future):
- ðŸ”œ Image generation (Imagen/DALL-E)
- ðŸ”œ Streaming responses cho UX tá»‘t hÆ¡n
- ðŸ”œ Redis cache thay in-memory
- ðŸ”œ Advanced retry logic
- ðŸ”œ A/B testing prompts
- ðŸ”œ Fine-tuning model (náº¿u cáº§n)

### Phase 3 (Scale):
- ðŸ”œ Multi-model support (GPT-4, Claude)
- ðŸ”œ Custom model training
- ðŸ”œ Distributed caching
- ðŸ”œ Real-time monitoring (Sentry, DataDog)

---

## Best Practices

### âœ… DO:
- Validate táº¥t cáº£ AI outputs
- Implement fallback cho má»i API calls
- Cache aggressively
- Monitor costs daily
- Test vá»›i nhiá»u tá»‰nh khÃ¡c nhau

### âŒ DON'T:
- Expose API key á»Ÿ client
- Skip validation
- Ignore error logs
- Over-optimize quÃ¡ sá»›m
- Trust AI output 100%

---

## TÃ i liá»‡u tham kháº£o

- [MegaLLM Documentation](https://ai.megallm.io/docs)
- [OpenAI SDK](https://www.npmjs.com/package/openai)
- [GPT-4o-mini Guide](https://platform.openai.com/docs/models/gpt-4o-mini)
- [MegaLLM Platform](https://ai.megallm.io)
